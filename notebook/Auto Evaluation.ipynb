{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cde96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path=\"..\"\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from models import SimCSE\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial, stats\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a966c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT Logging Lower than ERROR Level\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dba6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(device, model_path):\n",
    "    \"\"\"\n",
    "    Return Trained Tokenizer, Model\n",
    "    \"\"\"\n",
    "    # Model Size\n",
    "    model_size=model_path.split(\"/\")[-1].split(\"_\")[0].split(\"-\")[-1]\n",
    "\n",
    "    # Load Pre-Trained\n",
    "    if model_size==\"base\":\n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        pretrained=AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "    elif model_size==\"large\":\n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "        pretrained=AutoModel.from_pretrained(\"bert-large-uncased\").to(device)\n",
    "\n",
    "    # Load Trained\n",
    "    model=SimCSE(pretrained=pretrained)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model=model.to(device)\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d66c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_sts(device, model_path, split):\n",
    "    \"\"\"\n",
    "    Evaluate Trained Model on STS Benchmark\n",
    "    \"\"\"\n",
    "    # Load Trained Tokenizer, Model\n",
    "    tokenizer, model=load_trained_model(device=device, model_path=model_path)\n",
    "\n",
    "    # Load Dataset\n",
    "    if split==\"dev\":\n",
    "        dataset=open(\"../dataset/stsbenchmark/sts-dev.csv\", \"r\").read()\n",
    "    elif split==\"test\":\n",
    "        dataset=open(\"../dataset/stsbenchmark/sts-test.csv\", \"r\").read()\n",
    "\n",
    "    # Evaluate\n",
    "    preds=[]\n",
    "    labels=[]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataset.split(\"\\n\")[:-1]:\n",
    "            # Parse\n",
    "            label, sent1, sent2=data.split('\\t')[4:7]\n",
    "\n",
    "            # Encode\n",
    "            enc1=tokenizer.encode(sent1)\n",
    "            enc2=tokenizer.encode(sent2)\n",
    "\n",
    "            # Prediction\n",
    "            pred=1-spatial.distance.cosine(\n",
    "                np.array(model.get_embedding(torch.tensor([enc1]).to(device)).detach().cpu()),\n",
    "                np.array(model.get_embedding(torch.tensor([enc2]).to(device)).detach().cpu())\n",
    "            )\n",
    "            preds.append(pred)\n",
    "            # Labels\n",
    "            labels.append(float(label))\n",
    "\n",
    "    # Results\n",
    "    print(np.corrcoef(preds, labels))\n",
    "    print(stats.spearmanr(preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfea578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_casehold(device, model_path, split):\n",
    "    \"\"\"\n",
    "    Evaluate Trained Model on CaseHOLD\n",
    "    \"\"\"\n",
    "    # Load Trained Tokenizer, Model\n",
    "    tokenizer, model=load_trained_model(device=device, model_path=model_path)\n",
    "\n",
    "    # Load Dataset\n",
    "    if split==\"dev\":\n",
    "        dataset=load_dataset(\"lex_glue\", \"case_hold\")[\"validation\"]\n",
    "    elif split==\"test\":\n",
    "        dataset=load_dataset(\"lex_glue\", \"case_hold\")[\"test\"]\n",
    "\n",
    "    # Evaluate\n",
    "    preds=[]\n",
    "    labels=[]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            # Context\n",
    "            enc_context=tokenizer.encode(data[\"context\"])\n",
    "            embd_context=model.get_embedding(torch.tensor([enc_context]).to(device))\n",
    "\n",
    "            # Prediction\n",
    "            pred=-1\n",
    "            max_sim=-1\n",
    "            for idx, ending in enumerate(data[\"endings\"]):\n",
    "                # Ending\n",
    "                enc_ending=tokenizer.encode(ending)\n",
    "                #\n",
    "                sim=1-spatial.distance.cosine(\n",
    "                    np.array(embd_context.detach().cpu()),\n",
    "                    np.array(model.get_embedding(torch.tensor([enc_ending]).to(device)).detach().cpu())\n",
    "                )\n",
    "                #\n",
    "                if sim>max_sim:\n",
    "                    pred=idx\n",
    "                    max_sim=sim\n",
    "            preds.append(pred)\n",
    "            # Labels\n",
    "            labels.append(data[\"label\"])\n",
    "\n",
    "    # Results\n",
    "    print(classification_report(labels, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91b262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device=torch.device(\"cuda:3\")\n",
    "\n",
    "# Model Path\n",
    "model_name=\"simcse-base_general_batch64_lr7e-05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a9e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "simcse-base_general_batch64_lr7e-05_step250.pth\n",
      "---\n",
      "[[1.         0.62667342]\n",
      " [0.62667342 1.        ]]\n",
      "SpearmanrResult(correlation=0.6188642971916182, pvalue=2.8139713663700435e-159)\n",
      "===\n",
      "simcse-base_general_batch64_lr7e-05_step500.pth\n",
      "---\n",
      "[[1.         0.61957803]\n",
      " [0.61957803 1.        ]]\n",
      "SpearmanrResult(correlation=0.611379324054628, pvalue=1.8765526707007926e-154)\n",
      "===\n",
      "simcse-base_general_batch64_lr7e-05_step750.pth\n",
      "---\n",
      "[[1.         0.62852028]\n",
      " [0.62852028 1.        ]]\n",
      "SpearmanrResult(correlation=0.6213916921742345, pvalue=6.176909433078792e-161)\n",
      "===\n",
      "simcse-base_general_batch64_lr7e-05_step1000.pth\n",
      "---\n",
      "[[1.         0.61926282]\n",
      " [0.61926282 1.        ]]\n",
      "SpearmanrResult(correlation=0.6117845081520796, pvalue=1.036406400832974e-154)\n",
      "===\n",
      "simcse-base_general_batch64_lr7e-05_step1250.pth\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "for i in range(250, 100000, 250):\n",
    "    model_path=f'{model_name}_step{str(i)}.pth'\n",
    "    if model_path not in [ckpt for ckpt in os.listdir(\"../model/\") if model_name in ckpt]:\n",
    "        break\n",
    "    print(\"===\\n\"+model_path+\"\\n---\")\n",
    "        \n",
    "    evaluate_on_sts(\n",
    "    #evaluate_on_casehold(\n",
    "        device=device,\n",
    "        model_path=\"../model/\"+model_path,\n",
    "        split=\"dev\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a31860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
